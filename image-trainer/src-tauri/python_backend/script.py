import sys
import json
import time
import argparse
import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets, models, transforms

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
from torch.utils.data import DataLoader, Subset
import pandas as pd
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns

def main():
    parser = argparse.ArgumentParser(description='PyTorch Trainer')
    parser.add_argument('--path', type=str, required=True, help='Path to dataset')
    parser.add_argument('--epochs', type=int, default=5, help='Number of epochs')
    parser.add_argument('--save_path', type=str, required=False, help='Path to save models')
    parser.add_argument('--model', type=str, default='resnet18', choices=['resnet18', 'resnet50', 'efficientnet_b0', 'dcn', 'eva02', 'mobilenet_v3', 'vit_b_16', 'convnext'], help='Model type: resnet18, resnet50, efficientnet_b0, dcn, eva02, mobilenet_v3, vit_b_16, or convnext')
    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')
    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate for optimizer')
    parser.add_argument('--zip_dataset', action='store_true', help='Create a zip archive of the dataset')
    parser.add_argument('--only_zip', action='store_true', help='Exit after creating dataset zip')
    parser.add_argument('--num_workers', type=int, default=-1, help='Number of data loading workers (default: dynamic, set to 0 to disable multiprocessing)')
    parser.add_argument('--experiment_id', type=str, default=None, help='Unique experiment identifier (auto-generated by UI)')
    parser.add_argument('--patience', type=int, default=5, help='Early stopping patience (epochs without val loss improvement)')
    parser.add_argument('--resume', type=str, required=False, default=None, help='Path to a checkpoint .pth file to resume training from')
    args = parser.parse_args()
    
    data_dir = args.path
    save_dir = args.save_path if args.save_path else data_dir
    
    if not os.path.exists(data_dir):
        print(json.dumps({"status": "error", "message": "Directory not found"}), flush=True)
        return

    print("Initializing training...", flush=True)

    # Data Augmentation & Normalization
    data_transforms = {
        'train': transforms.Compose([
            transforms.Resize(256),
            # transforms.RandomResizedCrop(224), # Optional: Add more config for this later
            transforms.CenterCrop(224),
            transforms.RandomHorizontalFlip(), 
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'val': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    dataloaders = {}
    dataset_sizes = {}
    class_names = []
    
    # Check for existing split structure
    train_dir = os.path.join(data_dir, 'train')
    val_dir = os.path.join(data_dir, 'val')
    # Some datasets use 'validation' instead of 'val'
    if not os.path.exists(val_dir) and os.path.exists(os.path.join(data_dir, 'validation')):
        val_dir = os.path.join(data_dir, 'validation')
        
    test_dir = os.path.join(data_dir, 'test')

    batch_size = args.batch_size
    
    if args.num_workers >= 0:
        num_workers = args.num_workers
    else:
        try:
            cpu_count = os.cpu_count() or 1
            if sys.platform == 'win32':
                num_workers = min(4, cpu_count)
            else:
                num_workers = 4 if cpu_count > 4 else cpu_count
        except Exception:
            num_workers = 0

    if os.path.isdir(train_dir):
        print("Detected structured dataset (train/val/test).", flush=True)
        
        # Train
        train_dataset = datasets.ImageFolder(train_dir, data_transforms['train'])
        dataloaders['train'] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
        dataset_sizes['train'] = len(train_dataset)
        class_names = train_dataset.classes
        
        # Val
        if os.path.isdir(val_dir):
            val_dataset = datasets.ImageFolder(val_dir, data_transforms['val'])
            dataloaders['val'] = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
            dataset_sizes['val'] = len(val_dataset)
        else:
            print("Warning: No validation folder found.", flush=True)
            dataloaders['val'] = None
            dataset_sizes['val'] = 0
            
        # Test
        if os.path.isdir(test_dir):
            test_dataset = datasets.ImageFolder(test_dir, data_transforms['val'])
            dataloaders['test'] = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
            dataset_sizes['test'] = len(test_dataset)
        else:
            dataloaders['test'] = None
            dataset_sizes['test'] = 0
            
    else:
        print("Detected flat dataset. Performing auto-split (Train=80%, Val=10%, Test=10%).", flush=True)
        
        # 1. Check valid structure (subfolders)
        if not any(os.path.isdir(os.path.join(data_dir, i)) for i in os.listdir(data_dir)):
            print(json.dumps({
                "status": "error", 
                "message": "Invalid dataset structure. Expected folders for each class."
            }), flush=True)
            return

        # 2. Determine split indices
        # We load a dummy dataset just to get lengths and targets
        dummy_dataset = datasets.ImageFolder(data_dir)
        class_names = dummy_dataset.classes
        total_images = len(dummy_dataset)
        
        if total_images == 0:
            print(json.dumps({"status": "error", "message": "No images found."}), flush=True)
            return
            
        train_len = int(0.8 * total_images)
        val_len = int(0.1 * total_images)
        test_len = total_images - train_len - val_len
        
        # 3. Create datasets with correct transforms
        # We use train_test_split to get stratified indices
        from sklearn.model_selection import train_test_split
        
        targets = dummy_dataset.targets
        indices = list(range(total_images))
        
        try:
            train_idx, temp_idx, _, temp_targets = train_test_split(
                indices, targets, train_size=train_len, stratify=targets, random_state=42
            )
            val_idx, test_idx = train_test_split(
                temp_idx, train_size=val_len, stratify=temp_targets, random_state=42
            )
        except ValueError as e:
            print(f"Stratification failed ({e}), falling back to random split.", flush=True)
            from torch.utils.data import random_split
            subset_train, subset_val, subset_test = random_split(dummy_dataset, [train_len, val_len, test_len])
            train_idx = subset_train.indices
            val_idx = subset_val.indices
            test_idx = subset_test.indices
        
        # True datasets
        dataset_train_full = datasets.ImageFolder(data_dir, data_transforms['train'])
        dataset_eval_full = datasets.ImageFolder(data_dir, data_transforms['val']) # No aug for val/test
        
        train_dataset = Subset(dataset_train_full, train_idx)
        val_dataset = Subset(dataset_eval_full, val_idx)
        test_dataset = Subset(dataset_eval_full, test_idx)
        
        dataloaders['train'] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
        dataloaders['val'] = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
        dataloaders['test'] = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
        
        dataset_sizes['train'] = len(train_dataset)
        dataset_sizes['val'] = len(val_dataset)
        dataset_sizes['test'] = len(test_dataset)

    print(f"Classes: {class_names}", flush=True)
    print(f"Split sizes: Train={dataset_sizes.get('train',0)}, Val={dataset_sizes.get('val',0)}, Test={dataset_sizes.get('test',0)}", flush=True)

    # --- Zip Dataset (Optional) ---
    if args.zip_dataset or args.only_zip:
        import zipfile
        print("Creating dataset zip archive...", flush=True)
        zip_path = os.path.join(save_dir, 'dataset.zip')
        
        try:
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
                for phase, dataset in [('train', train_dataset), ('val', val_dataset), ('test', test_dataset)]:
                    if dataset is None: continue
                    if hasattr(dataset, '__len__') and len(dataset) == 0: continue
                    
                    # Resolve underlying dataset and indices
                    indices = range(len(dataset))
                    source_dataset = dataset
                    
                    if isinstance(dataset, Subset):
                        source_dataset = dataset.dataset
                        indices = dataset.indices
                    
                    # Check for samples attribute (ImageFolder standard)
                    if hasattr(source_dataset, 'samples'):
                        samples = source_dataset.samples
                        classes = source_dataset.classes
                        
                        for idx in indices:
                            img_path, class_idx = samples[idx]
                            class_name = classes[class_idx]
                            filename = os.path.basename(img_path)
                            
                            # Ensure unique arcname in case of duplicate filenames in different folders (rare but possible)
                            # We stick to phase/class/filename. If source had duplicates, they overwrite or error? 
                            # ZipFile allows duplicates.
                            arcname = f"{phase}/{class_name}/{filename}"
                            zf.write(img_path, arcname)
            
            print(json.dumps({
                "status": "dataset_zip",
                "message": "Dataset Zip Created",
                "path": zip_path
            }), flush=True)
            
        except Exception as e:
            print(f"Warning: Failed to create zip: {e}", flush=True)
            
        if args.only_zip:
            print("Export complete. Exiting.", flush=True)
            return

    # Setup Model
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}", flush=True)
    
    import model_factory
    
    try:
        model, parameters_to_optimize = model_factory.create_model(args.model, len(class_names), device)
    except ValueError as e:
        print(json.dumps({"status": "error", "message": str(e)}), flush=True)
        return

    try:
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.SGD(parameters_to_optimize, lr=args.learning_rate, momentum=0.9)
        
        num_epochs = args.epochs
        best_acc = 0.0
        start_epoch = 0

        # --- Checkpoint Resume ---
        if args.resume and os.path.isfile(args.resume):
            print(f"Resuming from checkpoint: {args.resume}", flush=True)
            checkpoint = torch.load(args.resume, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            best_acc = checkpoint.get('best_acc', 0.0)
            print(json.dumps({
                "status": "resumed",
                "message": f"Resumed from epoch {start_epoch}",
                "best_acc": f"{best_acc:.4f}"
            }), flush=True)
        elif args.resume:
            print(f"Warning: Checkpoint file not found at '{args.resume}', starting from scratch.", flush=True)

        # --- Early Stopping State ---
        best_val_loss = float('inf')
        epochs_no_improve = 0
        patience = args.patience

        print("Starting training loop...", flush=True)

        for epoch in range(start_epoch, num_epochs):
            for phase in ['train', 'val']:
                if dataset_sizes[phase] == 0:
                    continue # Skip empty phase

                if phase == 'train':
                    model.train()
                else:
                    model.eval()

                running_loss = 0.0
                running_corrects = 0

                for inputs, labels in dataloaders[phase]:
                    inputs = inputs.to(device)
                    labels = labels.to(device)

                    optimizer.zero_grad()

                    with torch.set_grad_enabled(phase == 'train'):
                        outputs = model(inputs)
                        _, preds = torch.max(outputs, 1)
                        loss = criterion(outputs, labels)

                        if phase == 'train':
                            loss.backward()
                            optimizer.step()

                    running_loss += loss.item() * inputs.size(0)
                    running_corrects += torch.sum(preds == labels.data)
                
                epoch_loss = running_loss / dataset_sizes[phase]
                epoch_acc = running_corrects.double() / dataset_sizes[phase]
                
                if phase == 'val':
                    # --- Save best model when accuracy improves ---
                    if epoch_acc > best_acc:
                        best_acc = epoch_acc
                        best_model_path = os.path.join(save_dir, 'best_model.pth')
                        torch.save(model.state_dict(), best_model_path)
                        print(json.dumps({
                            "status": "checkpoint",
                            "message": f"New Best Model! Acc: {epoch_acc:.4f}",
                            "path": best_model_path
                        }), flush=True)

                    # --- Full checkpoint (always, for resume support) ---
                    checkpoint_path = os.path.join(save_dir, 'checkpoint.pth')
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'best_acc': float(best_acc),
                    }, checkpoint_path)

                    # --- Early Stopping: track best val loss ---
                    if epoch_loss < best_val_loss:
                        best_val_loss = epoch_loss
                        epochs_no_improve = 0
                    else:
                        epochs_no_improve += 1

                    status_update = {
                        "epoch": epoch + 1,
                        "total_epochs": num_epochs,
                        "accuracy": f"{epoch_acc:.4f}",
                        "loss": f"{epoch_loss:.4f}",
                        "status": "training"
                    }
                    print(json.dumps(status_update), flush=True)

                    # --- Trigger early stop ---
                    if epochs_no_improve >= patience:
                        print(json.dumps({
                            "status": "stopped_early",
                            "epoch": epoch + 1,
                            "message": f"No val loss improvement for {patience} epochs. Stopping early."
                        }), flush=True)
                        break

            else:
                continue
            break

        print("Training Complete!", flush=True)
        
        # --- TEST / EVALUATION PHASE ---
        if dataloaders.get('test') and dataset_sizes['test'] > 0:
            print("Starting Evaluation on Test Set...", flush=True)
            
            # Load best weights
            best_model_path = os.path.join(save_dir, 'best_model.pth')
            if os.path.exists(best_model_path):
                model.load_state_dict(torch.load(best_model_path))
                print("Loaded best model weights.", flush=True)
            else:
                print("Warning: Best model not found, using last epoch weights.", flush=True)
                
            model.eval()
            
            all_preds = []
            all_labels = []
            
            with torch.no_grad():
                for inputs, labels in dataloaders['test']:
                    inputs = inputs.to(device)
                    labels = labels.to(device)
                    
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    
                    all_preds.extend(preds.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())
            
            # Generate Reports
            print("\n" + "="*30, flush=True)
            print("GENERATING RESULTS...", flush=True)
            print("="*30, flush=True)
            
            # 1. Classification Report (Dict for UI, Text for Logs)
            cr_text = classification_report(all_labels, all_preds, target_names=class_names, labels=range(len(class_names)), zero_division=0)
            print(cr_text, flush=True)
            
            cr_dict = classification_report(all_labels, all_preds, target_names=class_names, labels=range(len(class_names)), zero_division=0, output_dict=True)
            
            # 2. Confusion Matrix & Heatmap
            cm = confusion_matrix(all_labels, all_preds)
            
            # Plotting
            plt.figure(figsize=(10, 8))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
            plt.xlabel('Predicted')
            plt.ylabel('True')
            plt.title('Confusion Matrix')
            plt.tight_layout()
            
            cm_save_path = os.path.join(save_dir, 'confusion_matrix.png')
            plt.savefig(cm_save_path)
            plt.close()
            print(f"Confusion Matrix saved to: {cm_save_path}", flush=True)
            
            # Send Data to Frontend
            eval_result = {
                "status": "evaluation_complete",
                "report": cr_dict,
                "confusion_matrix_path": cm_save_path,
                "total_epochs": num_epochs,
                "test_size": dataset_sizes['test']
            }
            print(json.dumps(eval_result), flush=True)

    except Exception as e:
        # Catch and print any error clearly
        error_msg = {"status": "error", "message": f"Exception: {str(e)}"}
        print(json.dumps(error_msg), flush=True)
        # Also print to stderr for logs
        sys.stderr.write(f"Detailed Error: {str(e)}\n")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
